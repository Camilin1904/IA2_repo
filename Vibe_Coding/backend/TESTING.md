# ğŸ§ª GuÃ­a de Testing - QuickTask API

Esta guÃ­a explica cÃ³mo ejecutar y entender las pruebas automatizadas del proyecto QuickTask.

## ğŸ“‹ Tabla de Contenidos

1. [InstalaciÃ³n](#instalaciÃ³n)
2. [Estructura de Tests](#estructura-de-tests)
3. [Ejecutar Tests](#ejecutar-tests)
4. [Tipos de Tests](#tipos-de-tests)
5. [Fixtures](#fixtures)
6. [Cobertura de CÃ³digo](#cobertura-de-cÃ³digo)
7. [Buenas PrÃ¡cticas](#buenas-prÃ¡cticas)

---

## ğŸ”§ InstalaciÃ³n

### 1. Instalar dependencias de testing

```bash
# Instalar dependencias principales + testing
pip install -r requirements.txt
pip install -r test_requirements.txt

# O instalar todo junto
pip install -r requirements.txt -r test_requirements.txt
```

### 2. Verificar instalaciÃ³n

```bash
pytest --version
```

---

## ğŸ“ Estructura de Tests

```
backend/
â”œâ”€â”€ conftest.py          # ConfiguraciÃ³n global y fixtures compartidos
â”œâ”€â”€ pytest.ini           # ConfiguraciÃ³n de pytest
â”œâ”€â”€ test_crud.py         # Tests unitarios de operaciones CRUD
â”œâ”€â”€ test_api.py          # Tests de integraciÃ³n de endpoints
â”œâ”€â”€ test_schemas.py      # Tests de validaciÃ³n Pydantic
â””â”€â”€ TESTING.md           # Esta guÃ­a
```

### DescripciÃ³n de archivos

| Archivo | DescripciÃ³n | Tipo |
|---------|-------------|------|
| `conftest.py` | Define fixtures globales (BD en memoria, cliente de test) | ConfiguraciÃ³n |
| `test_crud.py` | Prueba lÃ³gica de negocio (CRUD operations) | Unitario |
| `test_api.py` | Prueba endpoints HTTP completos | IntegraciÃ³n |
| `test_schemas.py` | Prueba validaciÃ³n de datos con Pydantic | Unitario |

---

## â–¶ï¸ Ejecutar Tests

### Comandos bÃ¡sicos

```bash
# Ejecutar todos los tests
pytest

# Ejecutar con output verboso
pytest -v

# Ejecutar tests de un archivo especÃ­fico
pytest test_api.py

# Ejecutar una clase de tests especÃ­fica
pytest test_api.py::TestCreateTaskEndpoint

# Ejecutar un test especÃ­fico
pytest test_api.py::TestCreateTaskEndpoint::test_create_task_success

# Ejecutar tests que coincidan con un patrÃ³n
pytest -k "create"

# Detener en el primer error
pytest -x

# Mostrar prints durante los tests
pytest -s
```

### Ejecutar por categorÃ­a

```bash
# Solo tests unitarios
pytest test_crud.py test_schemas.py

# Solo tests de integraciÃ³n
pytest test_api.py

# Tests relacionados con creaciÃ³n
pytest -k "create"

# Tests relacionados con actualizaciÃ³n
pytest -k "update"
```

---

## ğŸ” Tipos de Tests

### 1. Tests Unitarios (`test_crud.py`)

Prueban la **lÃ³gica de negocio** de forma aislada.

**Ejemplo:**
```python
def test_create_task_minimal(test_db):
    """Crear tarea solo con tÃ­tulo"""
    task_data = TaskCreate(title="Llamar al mÃ©dico")
    task = crud.create_task(test_db, task_data)
    
    assert task.id is not None
    assert task.title == "Llamar al mÃ©dico"
```

**Cobertura:**
- âœ… Crear tareas (completas y mÃ­nimas)
- âœ… Leer tareas (por ID, todas, con filtros)
- âœ… Actualizar tareas (parcial y completa)
- âœ… Eliminar tareas
- âœ… BÃºsqueda y filtrado
- âœ… PaginaciÃ³n

### 2. Tests de IntegraciÃ³n (`test_api.py`)

Prueban **endpoints HTTP completos** (request â†’ response).

**Ejemplo:**
```python
def test_create_task_success(client):
    """Crear tarea vÃ­a POST /tasks"""
    response = client.post("/tasks", json={
        "title": "Nueva tarea",
        "completed": False
    })
    
    assert response.status_code == 201
    assert response.json()["title"] == "Nueva tarea"
```

**Cobertura:**
- âœ… GET `/tasks` (listar con filtros)
- âœ… GET `/tasks/{id}` (obtener por ID)
- âœ… POST `/tasks` (crear)
- âœ… PUT `/tasks/{id}` (actualizar completa)
- âœ… PATCH `/tasks/{id}` (actualizar parcial)
- âœ… DELETE `/tasks/{id}` (eliminar)
- âœ… Casos de error (404, 422)
- âœ… Flujos completos (E2E)

### 3. Tests de ValidaciÃ³n (`test_schemas.py`)

Prueban **validaciÃ³n de datos con Pydantic**.

**Ejemplo:**
```python
def test_empty_title_raises_error():
    """TÃ­tulo vacÃ­o debe fallar"""
    with pytest.raises(ValidationError):
        TaskCreate(title="")
```

**Cobertura:**
- âœ… Campos obligatorios
- âœ… LÃ­mites de longitud
- âœ… Tipos de datos
- âœ… Valores por defecto

---

## ğŸ¯ Fixtures

Los **fixtures** son funciones reutilizables que preparan el entorno de test.

### Fixtures disponibles (en `conftest.py`)

#### 1. `test_db`
Proporciona una **base de datos SQLite en memoria**.

```python
def test_example(test_db):
    # test_db es una sesiÃ³n de BD temporal
    task = crud.create_task(test_db, TaskCreate(title="Test"))
    assert task.id is not None
```

**CaracterÃ­sticas:**
- Se crea nueva para cada test (aislamiento)
- Se destruye automÃ¡ticamente despuÃ©s del test
- No persiste datos entre tests

#### 2. `client`
Proporciona un **cliente de prueba de FastAPI**.

```python
def test_example(client):
    # client simula peticiones HTTP
    response = client.get("/tasks")
    assert response.status_code == 200
```

**CaracterÃ­sticas:**
- Usa la BD en memoria automÃ¡ticamente
- No hace peticiones HTTP reales
- MÃ¡s rÃ¡pido que peticiones reales

#### 3. `sample_task_data`
Proporciona **datos de ejemplo** para crear tareas.

```python
def test_example(sample_task_data):
    # sample_task_data es un diccionario con datos vÃ¡lidos
    assert sample_task_data["title"] == "Tarea de prueba"
```

#### 4. `create_sample_task`
Crea una **tarea de ejemplo** y retorna la respuesta.

```python
def test_example(create_sample_task):
    # create_sample_task ya existe en la BD
    task_id = create_sample_task["id"]
    # Ahora puedes usarla para tests de actualizaciÃ³n/eliminaciÃ³n
```

---

## ğŸ“Š Cobertura de CÃ³digo

### Ejecutar con reporte de cobertura

```bash
# Generar reporte de cobertura
pytest --cov=. --cov-report=html

# Ver reporte en terminal
pytest --cov=. --cov-report=term-missing

# Solo mostrar archivos con menos del 100%
pytest --cov=. --cov-report=term-missing:skip-covered
```

### Ver reporte HTML

```bash
# Generar y abrir en navegador
pytest --cov=. --cov-report=html
open htmlcov/index.html  # En Mac/Linux
# start htmlcov/index.html  # En Windows
```

### Interpretar resultados

```
Name           Stmts   Miss  Cover
----------------------------------
main.py           45      2    96%
crud.py           38      0   100%
models.py         12      0   100%
schemas.py        25      1    96%
----------------------------------
TOTAL            120      3    98%
```

- **Stmts**: LÃ­neas de cÃ³digo
- **Miss**: LÃ­neas no ejecutadas
- **Cover**: Porcentaje de cobertura

---

## âœ… Buenas PrÃ¡cticas

### 1. Nomenclatura de tests

```python
# âœ… Bueno - Describe claramente quÃ© se prueba
def test_create_task_with_all_fields():
    pass

def test_update_task_title_only():
    pass

# âŒ Malo - Nombre poco descriptivo
def test_task1():
    pass

def test_update():
    pass
```

### 2. OrganizaciÃ³n con clases

```python
class TestCreateTask:
    """Agrupa todos los tests de creaciÃ³n"""
    
    def test_create_success(self):
        pass
    
    def test_create_without_title(self):
        pass
```

### 3. Assertions claras

```python
# âœ… Bueno - Assertion especÃ­fica
assert response.status_code == 201
assert task.title == "Nueva tarea"

# âŒ Malo - Assertion genÃ©rica
assert response
assert task
```

### 4. Arrange-Act-Assert (AAA)

```python
def test_create_task():
    # Arrange (preparar)
    task_data = TaskCreate(title="Test")
    
    # Act (ejecutar)
    task = crud.create_task(db, task_data)
    
    # Assert (verificar)
    assert task.id is not None
```

### 5. Tests independientes

```python
# âœ… Bueno - Cada test crea sus datos
def test_a(client):
    task = client.post("/tasks", json={"title": "Tarea A"})
    assert task.status_code == 201

def test_b(client):
    task = client.post("/tasks", json={"title": "Tarea B"})
    assert task.status_code == 201

# âŒ Malo - test_b depende de test_a
def test_a(client):
    global task_id
    task = client.post("/tasks", json={"title": "Tarea"})
    task_id = task.json()["id"]

def test_b(client):
    # Asume que test_a ya corriÃ³
    response = client.get(f"/tasks/{task_id}")
```

---

## ğŸ“ Ejemplos de Uso

### Caso 1: Probar solo la creaciÃ³n

```bash
pytest -k "create" -v
```

### Caso 2: Probar endpoint especÃ­fico

```bash
pytest test_api.py::TestCreateTaskEndpoint -v
```

### Caso 3: Debug de un test que falla

```bash
# Ver mÃ¡s detalles del error
pytest test_api.py::TestCreateTaskEndpoint::test_create_task_success -vv

# Ver prints
pytest test_api.py::TestCreateTaskEndpoint::test_create_task_success -s
```

### Caso 4: Ejecutar tests rÃ¡pidos

```bash
# Omitir tests marcados como lentos
pytest -m "not slow"
```

---

## ğŸ› Troubleshooting

### Error: "No module named 'fastapi'"

```bash
# Instalar dependencias
pip install -r requirements.txt -r test_requirements.txt
```

### Error: "database is locked"

AsegÃºrate de usar `sqlite:///:memory:` en tests (ya estÃ¡ configurado en `conftest.py`).

### Tests pasan individualmente pero fallan juntos

Verifica que los tests sean independientes (no compartan estado).

---

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n pytest](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)

---

## ğŸ“Š Resumen de Cobertura Actual

| Componente | Tests | Cobertura Estimada |
|------------|-------|-------------------|
| CRUD Operations | 21 tests | ~95% |
| API Endpoints | 25 tests | ~90% |
| Schemas | 11 tests | ~85% |
| **TOTAL** | **57 tests** | **~90%** |

---

**Â¡Happy Testing!** ğŸš€âœ…

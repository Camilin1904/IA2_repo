# ðŸš€ Quick Start - Testing QuickTask API

## âš¡ Inicio RÃ¡pido (3 pasos)

### 1ï¸âƒ£ Instalar
```bash
cd backend
pip install -r requirements.txt -r test_requirements.txt
```

### 2ï¸âƒ£ Ejecutar
```bash
pytest -v
```

### 3ï¸âƒ£ Ver Cobertura
```bash
pytest --cov=. --cov-report=html
open htmlcov/index.html
```

---

## ðŸ“¸ Ejemplos de EjecuciÃ³n

### Ejemplo 1: Todos los tests
```bash
$ pytest

======================== test session starts =========================
collected 57 items

test_crud.py::TestCreateTask::test_create_task_with_all_fields PASSED [ 1%]
test_crud.py::TestCreateTask::test_create_task_minimal PASSED        [ 3%]
test_api.py::TestCreateTaskEndpoint::test_create_task_success PASSED [ 5%]
...

===================== 57 passed in 2.34s ============================
```

### Ejemplo 2: Solo tests de creaciÃ³n
```bash
$ pytest -k "create" -v

======================== test session starts =========================
test_crud.py::TestCreateTask::test_create_task_with_all_fields PASSED
test_crud.py::TestCreateTask::test_create_task_minimal PASSED
test_crud.py::TestCreateTask::test_create_multiple_tasks PASSED
test_api.py::TestCreateTaskEndpoint::test_create_task_success PASSED
test_api.py::TestCreateTaskEndpoint::test_create_task_minimal PASSED
test_api.py::TestCreateTaskEndpoint::test_create_task_without_title PASSED
test_api.py::TestCreateTaskEndpoint::test_create_task_empty_title PASSED
test_schemas.py::TestTaskCreateSchema::test_valid_task_creation PASSED

===================== 8 passed in 0.45s =============================
```

### Ejemplo 3: Con reporte de cobertura
```bash
$ pytest --cov=. --cov-report=term-missing

======================== test session starts =========================
collected 57 items

test_crud.py ........................                                [ 42%]
test_api.py ............................                             [ 91%]
test_schemas.py .....                                                [100%]

---------- coverage: platform linux, python 3.10.12 -----------
Name            Stmts   Miss  Cover   Missing
---------------------------------------------
main.py            45      2    96%   158-159
crud.py            38      0   100%
models.py          12      0   100%
schemas.py         25      1    96%   42
database.py        15      0   100%
---------------------------------------------
TOTAL             135      3    98%

===================== 57 passed in 2.34s ============================
```

---

## ðŸŽ¯ Casos de Uso Comunes

### âœ… Test un endpoint especÃ­fico
```bash
pytest test_api.py::TestCreateTaskEndpoint::test_create_task_success -v
```

### âœ… Test una clase completa
```bash
pytest test_api.py::TestCreateTaskEndpoint -v
```

### âœ… Solo tests unitarios
```bash
pytest test_crud.py test_schemas.py -v
```

### âœ… Solo tests de integraciÃ³n
```bash
pytest test_api.py -v
```

### âœ… Detener en el primer error
```bash
pytest -x
```

### âœ… Ver prints durante tests
```bash
pytest -s
```

### âœ… Modo silencioso
```bash
pytest -q
```

---

## ðŸ› ï¸ Usar el Script Helper

```bash
# Dar permisos (solo la primera vez)
chmod +x run_tests.sh

# Ejecutar todos los tests
./run_tests.sh

# Solo tests unitarios
./run_tests.sh unit

# Solo tests de integraciÃ³n
./run_tests.sh integration

# Con reporte de cobertura
./run_tests.sh coverage

# Modo rÃ¡pido
./run_tests.sh fast

# Ver ayuda
./run_tests.sh help
```

---

## ðŸ“Š Estructura Visual de Tests

```
backend/
â”‚
â”œâ”€â”€ ðŸ“ conftest.py          â† Fixtures globales
â”‚   â”œâ”€â”€ test_db            â†’ BD en memoria
â”‚   â”œâ”€â”€ client             â†’ TestClient FastAPI
â”‚   â”œâ”€â”€ sample_task_data   â†’ Datos de ejemplo
â”‚   â””â”€â”€ create_sample_task â†’ Tarea precreada
â”‚
â”œâ”€â”€ ðŸ”§ test_crud.py         â† Tests Unitarios (21 tests)
â”‚   â”œâ”€â”€ TestCreateTask     â†’ 3 tests
â”‚   â”œâ”€â”€ TestReadTask       â†’ 4 tests
â”‚   â”œâ”€â”€ TestFilterTasks    â†’ 4 tests
â”‚   â”œâ”€â”€ TestUpdateTask     â†’ 4 tests
â”‚   â”œâ”€â”€ TestDeleteTask     â†’ 3 tests
â”‚   â””â”€â”€ TestDeleteAndCount â†’ 3 tests
â”‚
â”œâ”€â”€ ðŸŒ test_api.py          â† Tests IntegraciÃ³n (25 tests)
â”‚   â”œâ”€â”€ TestRootEndpoint        â†’ 2 tests
â”‚   â”œâ”€â”€ TestCreateTaskEndpoint  â†’ 5 tests
â”‚   â”œâ”€â”€ TestListTasksEndpoint   â†’ 5 tests
â”‚   â”œâ”€â”€ TestGetTaskByIdEndpoint â†’ 2 tests
â”‚   â”œâ”€â”€ TestUpdateTaskEndpoint  â†’ 6 tests
â”‚   â”œâ”€â”€ TestDeleteTaskEndpoint  â†’ 3 tests
â”‚   â””â”€â”€ TestCompleteWorkflow    â†’ 2 tests
â”‚
â””â”€â”€ âœ… test_schemas.py      â† Tests ValidaciÃ³n (11 tests)
    â”œâ”€â”€ TestTaskCreateSchema â†’ 6 tests
    â”œâ”€â”€ TestTaskUpdateSchema â†’ 4 tests
    â””â”€â”€ TestTaskResponseSchema â†’ 3 tests
```

---

## ðŸ’¡ Tips y Trucos

### ðŸ” Debug de un test que falla
```bash
# Ver traceback completo
pytest test_api.py::test_create_task_success -vv

# Ver prints y variables
pytest test_api.py::test_create_task_success -s

# Entrar en debugger al fallar
pytest --pdb
```

### ðŸ“ˆ AnÃ¡lisis de cobertura
```bash
# Generar reporte HTML
pytest --cov=. --cov-report=html

# Solo mostrar lÃ­neas no cubiertas
pytest --cov=. --cov-report=term-missing

# Ignorar archivos de test en cobertura
pytest --cov=. --cov-report=term --cov-config=.coveragerc
```

### âš¡ Optimizar velocidad
```bash
# Ejecutar tests en paralelo (requiere pytest-xdist)
pip install pytest-xdist
pytest -n auto

# Ejecutar solo tests que fallaron la Ãºltima vez
pytest --lf

# Ejecutar primero los que fallaron
pytest --ff
```

---

## ðŸ§ª AnatomÃ­a de un Test

```python
def test_create_task_success(client):
    """DescripciÃ³n clara del test"""
    
    # 1. ARRANGE - Preparar datos
    task_data = {
        "title": "Nueva tarea",
        "completed": False
    }
    
    # 2. ACT - Ejecutar acciÃ³n
    response = client.post("/tasks", json=task_data)
    
    # 3. ASSERT - Verificar resultado
    assert response.status_code == 201
    assert response.json()["title"] == "Nueva tarea"
```

---

## ðŸŽ“ Conceptos Clave

### âœ¨ Fixtures
```python
@pytest.fixture
def client(test_db):
    """Cliente de test con BD en memoria"""
    # Setup
    app.dependency_overrides[get_db] = lambda: test_db
    client = TestClient(app)
    
    yield client  # â† Test usa este cliente
    
    # Teardown automÃ¡tico
```

### ðŸ”’ Aislamiento de Tests
- Cada test usa una **BD nueva en memoria**
- No comparten estado entre tests
- Orden de ejecuciÃ³n no importa

### âš¡ Base de Datos en Memoria
```python
# Mucho mÃ¡s rÃ¡pido que SQLite en disco
SQLALCHEMY_TEST_DATABASE_URL = "sqlite:///:memory:"
```

---

## ðŸ“š DocumentaciÃ³n Relacionada

| Documento | Contenido |
|-----------|-----------|
| [TESTING.md](TESTING.md) | GuÃ­a completa de testing |
| [TEST_SUMMARY.md](TEST_SUMMARY.md) | Resumen detallado de todos los tests |
| [README.md](README.md) | DocumentaciÃ³n general de la API |

---

## âœ… Checklist de Testing

Antes de hacer commit:

- [ ] Todos los tests pasan: `pytest`
- [ ] Cobertura > 90%: `pytest --cov=.`
- [ ] Sin warnings: `pytest --strict-warnings`
- [ ] Code style: `black .` (opcional)
- [ ] Linting: `flake8 .` (opcional)

---

## ðŸ†˜ Troubleshooting

### âŒ Error: "No module named 'pytest'"
```bash
pip install -r test_requirements.txt
```

### âŒ Error: "database is locked"
Verifica que uses `sqlite:///:memory:` en tests (ya configurado en `conftest.py`).

### âŒ Tests pasan individualmente pero fallan juntos
Los tests no son independientes. Verifica que no compartan estado.

### âŒ Import errors
```bash
# AsegÃºrate de estar en el directorio correcto
cd backend
pytest
```

---

**Â¡Feliz Testing!** ðŸš€âœ¨

---

> ðŸ’¡ **Tip**: Para validar cambios rÃ¡pidamente, usa:
> ```bash
> pytest -x -v  # Detener en primer error con output verbose
> ```
